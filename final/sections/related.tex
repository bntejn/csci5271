% sections/related.tex
% 3. Related Works

This section surveys attacks/ defenses on/against anonymizing systems, especially Tor. 

\subsection{Fingerprinting Attacks}
The idea of extracting information about the content of encrypted SSL packets dates back to 1996 \cite{wagner96}. Later in 2002, Hintz presented an attack on an encrypting web proxy calling it website fingerprinting. The attack successfully exploited a traffic analysis-based vulnerability to detect which website a particular user is surfing \cite{hintz2003}. Since then, a lot of research has been conducted in this area trying either to propose a more realistic attack, or a more effective defense mechanism.

One interesting study has been done in \cite{herrmann2009}, where authors lunched WF attack on different anonymizing techniques, including OpenSSH, OpenVPN, Stunnel, Tor. They tried to attack 775 different websites. The result presents a low detection rate in the closed world attack model (only a 3\% success rate for 775 pages). They used Multinomial Naive Bayes classifier focusing only on packet sizes. Later on, this attack has been improved with around 90\% accuracy for 100 wbpages in \cite{wang2013improved, cai2012touching}. In \cite{cai2012touching}, authors could defeat ad hoc defense mechanisms describing a new defense scheme that provides provable security properties. Penchenko et al. \cite{panchenko11} were also among the first to report website fingerprinting attacks with reasonable accuracy on Tor. They provided a sufficient understanding of the feature set and classification framework required for this attack. In fact, they carried out a comprehensive study of WF on Tor, which considers both closed world and open world attack models.

Generally speaking, attacks on anonymizing networks take a variety of approaches; some of the attacks tries to discover the identity of the anonymous user, others focus on uncovering the private pathway, and others attempt to identify the servers users interact with  \cite{cai2012touching}.

\subsection{Defense Mechanisms}
Defense mechanisms are usually developed at the IP/TCP level by changing the pattern of traffic. For example, they split packets into multiple packets, insert fake packets into the traffic, or involve padding packets. A study performed in \cite{fu2003} shows that transmission at random intervals could be a defense against analysis-based attacks. Another defense technique proposed in \cite{wright2009}, called traffic morphing, uses some tricks to change a traffic pattern in a way that it looks like another pattern. However, this method does not split or disordered packets. Therefore, attacks that work without packet size information can easily defeat this mechanism.

In \cite{luo2011}, Luo et al. proposed a collection of tricks at the HTTP/TCP level as a defense mechanism against some analysis attacks. For example, they changed window sizes and the order of packets in the TCP stream. Besides, at the HTTP level, they tried to insert some extra data into HTTP GET headers, generate some irrelevant HTTP request, and re-order requests. The Tor community also introduced "randomized pipelining" \cite{perry11} as a defense mechanism, where the browser loads the web content in a random order. Despite such techniques, the proposed attack in \cite{cai2012touching} managed to successfully recognize target web pages with the accuracy of 87\%. The experiments show that techniques like traffic morphing and randomized pipelining can impose high costs, but are unable to stop the WF attack. This attack is able to ignore packet sizes, while most of the attacks on Tor work based on packet sizes.

\subsection{Criticism}
In the literature, some work always debate over the practical feasibility of the WF attack. This section tries to summarize issues questioning the practicality of WF.

There are many different factors that play a role in the success of WF. The main criticism is that academic papers usually oversimplify WF attack models by making some unrealistic assumptions over users' browsing habits, training and testing traces, even the version of Tor used for testing/ training. In \cite{juarez14}, it is discussed that the studies performed in \cite{cai2012touching, herrmann2009, panchenko11, wang2013improved, shi2009} simplify the problem and overestimate the adversary's capabilities.

Another criticism is that all works lunch attacks on individual pages, instead of overall websites. So, although the attack is called website fingerprinting, it is actually about webpage fingerprinting. In addition, there are some main factors, including the the hypothesis state space and the size of the instance space, that affect the accuracy of a classifier. Even, the number of training samples provided to the classifier and false positive rates matters. Since reliable feature information is constant, with the increase in the number of classification categories, the classifier eventually runs out of descriptive feature information, which causes either true positive accuracy goes down or the false positive rate goes up. \cite{TorBlog} discusses that the effects of such factors are quite observable in the papers with a sufficient world size.

Although more attention should be paid to the scenarios by which attacks are evaluated, we should not dismiss WF as a threat. However, \cite{TorBlog} argues that, due to theoretical and practical issues, realistic WF attacks are hard to lunch on Tor. Therefore, it claims that even simple defenses could protect Tor users against WF. It seems that the Tor community believes that defense mechanisms do not need to be very complicated to be effective.

To the best of our knowledge, none of the defense techniques has investigated the effect of link pre-fecting on WF. In the next section, we will describe the concept of link pre-fetching. Since most of the defenses are applied at the Tor network, they are required to be acceptable by the Tor community. However, we aim to propose a defense technique which can also be used by the website owners. 

