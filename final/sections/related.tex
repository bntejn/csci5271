% sections/related.tex
% 3. Related Works

This section surveys attacks/ defenses on/against anonymizing systems, especially Tor. 

\subsection{Fingerprinting Attacks}
The idea of inferring meaningful information based on sniffing and analyzing encrypted SSL packets, which is called deep packet inspection (DPI),  was introduced in 1996 \cite{wagner96}. Later in 2002, Hintz presented the first website fingerprinting attack on an encrypted web proxy. The attack shows how DPI is powerful enough to identify a specific web site that the user is surfing \cite{hintz2003}. Since then, a lot of research has been conducted in this area trying either to propose a more realistic WF attack, or a more effective defense mechanism against WF.

Herrmann et al. \cite{herrmann2009} pointed out that widely used anonymous networks such as Tor still failed to defend against WF attack. They achieved only a 3\% success rate for 775 pages using Multinomial Naive Bayes classifier focusing only on packet sizes. The WF attacks on Tor suggested by Penchenko et al. \cite{panchenko11} showed more reasonable accuracy with more sophisticated machine learning techniques and diverse features. Later on, based on \cite{panchenko11}'s attack model, many works such as \cite{wang2013improved, cai2012touching}   tried to build more powerful and realistic attacks that improved the accuracy rate up to around 90\% in closed word setting. They did thorough inverstigation on extracting powerful features such as Tor cells and improving classification method. Especially, Cai et al. \cite{cai2012touching} had shown that their novel WF attack successfully defeated existing well-known defense mechanisms such as HTTPOS \cite{luo2011}. However, even though existing works have successfully improved the effectiveness of WF attacks on Tor, underlying assumptions for experiments significantly simplified real world setting.  As Juarez et al. \cite{juarez14} pointed out that existing WF attacks are unrealistic with assumptions such as the client setting, we decided to focus on building more robust and practical defense mechanisim instead of developing advanced attack models to improve the accuracy rate.
\textbf{please add  or modify the comparison of our work to existing works}
%Generally speaking, attacks on anonymizing networks take a variety of approaches; some of the attacks tries to discover the identity of the anonymous user, others focus on uncovering the private pathway, and others attempt to identify the servers users interact with  \cite{cai2012touching}.

\subsection{Defense Mechanisms}
Defense mechanisms are usually developed at the IP/TCP level by changing the pattern of traffic. For example, they splited packets into multiple packets, injected spurious packets into the traffic, or involved  padding packets to prevent from leaking features of traffic. A study performed in \cite{fu2003} suggested to transmit packets at random intervals as a defense mechanism against traffic analysis. Another defense technique proposed in \cite{wright2009}, called traffic morphing, uses some tricks to change a traffic pattern to disguise new or existing other traffic. However, since their approach still leaks the information such as packet order, attacks that work without packet size information can easily defeat their mechanism.

In \cite{luo2011}, Luo et al. proposed novel HTTP/TCP level defense mechanisms against some analysis attacks by changing window sizes and the order of packets in the TCP stream. Besides, at the HTTP level, they tried to inject some extra data into HTTP GET headers, generate some irrelevant HTTP request, and re-order requests. The Tor community also introduced "randomized pipelining" \cite{perry11} to defend the WF, by having the browser load the web content in a random order. Despite such techniques, the proposed attack in \cite{cai2012touching} managed to successfully recognize target web pages with the accuracy of 87\%. 
Howerver, all exisitng defenses have not been neither efficient nor effective for fancy WF attacks. %This attack is able to ignore packet sizes, while most of the attacks on Tor work based on packet sizes.
In contrast, our work more focuses on utilizing existing HTML systax that is link prefetching shown in Section 4, which subsequently does not accomodate extra cost.
\textbf{please add  or modify the comparison of our work to existing works}

\subsection{Criticism}
In the literature, some work always debate over the practical feasibility of the WF attack. This section tries to summarize issues questioning the practicality of WF.

There are many different factors that play a role in the success of WF. The main criticism is that academic papers usually oversimplify WF attack models by making some unrealistic assumptions over users' browsing habits, training and testing traces, even the version of Tor used for testing/ training. In \cite{juarez14}, it is discussed that the studies performed in \cite{cai2012touching, herrmann2009, panchenko11, wang2013improved, shi2009} simplify the problem and overestimate the adversary's capabilities.

Another criticism is that all works lunch attacks on individual pages, instead of overall websites. So, although the attack is called website fingerprinting, it is actually about webpage fingerprinting. In addition, there are some main factors, including the the hypothesis state space and the size of the instance space, that affect the accuracy of a classifier. Even, the number of training samples provided to the classifier and false positive rates matters. Since reliable feature information is constant, with the increase in the number of classification categories, the classifier eventually runs out of descriptive feature information, which causes either true positive accuracy goes down or the false positive rate goes up. \cite{TorBlog} discusses that the effects of such factors are quite observable in the papers with a sufficient world size.

Although more attention should be paid to the scenarios by which attacks are evaluated, we should not dismiss WF as a threat. However, \cite{TorBlog} argues that, due to theoretical and practical issues, realistic WF attacks are hard to lunch on Tor. Therefore, it claims that even simple defenses could protect Tor users against WF. It seems that the Tor community believes that defense mechanisms do not need to be very complicated to be effective.

To the best of our knowledge, none of the defense techniques has investigated the effect of link pre-fecting on WF. In the next section, we will describe the concept of link pre-fetching. Since most of the defenses are applied at the Tor network, they are required to be acceptable by the Tor community. However, we aim to propose a defense technique which can also be used by the website owners. 
\subsection{Web Prefetching}
Web prefetching has been widely used to reduce the latency for users. Fan et al.\cite{Fan1999} introduced the first prefetching between caching proxies and clients. Chen et al. \cite{Chen2003} pre-fetched web pages that will be visited near future. In addition, there have been several works using prefetching over anonymous network. Nguyen et al. \cite{} prefetched web components of web page, which users are visiting, to solve the delay performance of Tor. They set two proxies, one sits between the client and browser, and the other sits between the exit relay and the website. Even though they successfully improved tor performance, the extra setting of proxies and the communication over them are not good in terms of security since we need to trust them. (e.g., proxies are compromised by adversaries). In contrast, we do not enforce any additional setting on tor network as well as client-side to hinder from such additional security risks.  \textbf{please add  or modify the comparison of our work to existing works}


